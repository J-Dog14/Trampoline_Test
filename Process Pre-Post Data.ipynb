{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:16:58.031171Z",
     "start_time": "2025-01-06T23:16:51.421999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "# Ask user to select folder containing the Session XML file\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "selected_folder = filedialog.askdirectory(initialdir='D:/Tramp Test/Data/')\n",
    "if not selected_folder:\n",
    "    raise ValueError(\"No folder was selected.\")\n",
    "\n",
    "# Extract the test_date from the selected folder name\n",
    "folder_name = os.path.basename(selected_folder)\n",
    "test_date = folder_name.split('_', 1)[0]  # Extract '2024-08-13' from '2024-08-13_105_Growth Plate_'\n",
    "\n",
    "# Find the XML file titled \"Session\" in the selected folder\n",
    "xml_file_path = ''\n",
    "for r, dirs, files in os.walk(selected_folder):\n",
    "    for file in files:\n",
    "        if file.lower().startswith('session') and file.lower().endswith('.xml'):\n",
    "            xml_file_path = os.path.join(r, file)\n",
    "            break\n",
    "    if xml_file_path:\n",
    "        break\n",
    "\n",
    "if not xml_file_path:\n",
    "    raise FileNotFoundError(\"No 'Session' XML file found in the selected folder.\")\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "root_xml = tree.getroot()\n",
    "\n",
    "# Extract required fields from XML\n",
    "name = root_xml.find(\".//Name\").text\n",
    "dob = root_xml.find(\".//DOB\").text\n",
    "height = root_xml.find(\".//Height\").text\n",
    "weight = root_xml.find(\".//Weight\").text\n",
    "pre_post = root_xml.find(\".//Pre_Post\").text.lower()\n",
    "exp_control = root_xml.find(\".//Exp_Control\").text.lower()\n",
    "creation_date = root_xml.find(\".//Creation_date\").text\n",
    "comments = root_xml.find(\".//Comments\").text\n",
    "\n",
    "# Calculate age from DOB\n",
    "dob_date = datetime.strptime(dob, \"%Y-%m-%d\")\n",
    "today = datetime.today()\n",
    "age = today.year - dob_date.year - ((today.month, today.day) < (dob_date.month, dob_date.day))\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = 'D:/Tramp Test/Tramp_Test.sqlite'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create necessary tables\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS Participants (\n",
    "    participant_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    dob DATE,\n",
    "    height REAL,\n",
    "    weight REAL,\n",
    "    participant_group TEXT -- Simplified to exp or control\n",
    ")''')\n",
    "\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS Sessions (\n",
    "    session_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    participant_id INTEGER,\n",
    "    test_date DATE,\n",
    "    pre_post TEXT,\n",
    "    exp_control TEXT,\n",
    "    comments TEXT,\n",
    "    FOREIGN KEY (participant_id) REFERENCES Participants(participant_id)\n",
    ")''')\n",
    "\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS Movements (\n",
    "    movement_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    movement_name TEXT\n",
    ")''')\n",
    "\n",
    "# Update Results table structure to include pre_post\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Results (\n",
    "        result_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Trial_Num INTEGER,\n",
    "        pre_post TEXT, -- pre or post\n",
    "        name TEXT,\n",
    "        movement TEXT,\n",
    "        JH_IN REAL,\n",
    "        LEWIS_PEAK_POWER REAL,\n",
    "        NORM_LEWIS_PEAK_POWER_KG REAL,\n",
    "        Max_Force REAL\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Insert participant if not already in the database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT participant_id FROM Participants WHERE name = ?\", (name,))\n",
    "participant = cursor.fetchone()\n",
    "if participant is None:\n",
    "    # Assign participant_group as exp or control only\n",
    "    participant_group = exp_control  # exp or control\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Participants (name, dob, height, weight, participant_group) \n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", (name, dob, height, weight, participant_group))\n",
    "    participant_id = cursor.lastrowid\n",
    "else:\n",
    "    participant_id = participant[0]\n",
    "\n",
    "print(f\"Participant '{name}' assigned to group '{participant_group}' with participant_id: {participant_id}\")\n",
    "\n",
    "# Insert session\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO Sessions (participant_id, test_date, pre_post, exp_control, comments) \n",
    "    VALUES (?, ?, ?, ?, ?)\n",
    "\"\"\", (participant_id, test_date, pre_post, exp_control, comments))\n",
    "session_id = cursor.lastrowid\n",
    "\n",
    "# Define movements and ensure they are in the Movements table\n",
    "movements = ['cmj', 'dj', 'ppu']\n",
    "movement_ids = {}\n",
    "for movement in movements:\n",
    "    cursor.execute(\"SELECT movement_id FROM Movements WHERE movement_name = ?\", (movement,))\n",
    "    result = cursor.fetchone()\n",
    "    if result is None:\n",
    "        cursor.execute(\"INSERT INTO Movements (movement_name) VALUES (?)\", (movement,))\n",
    "        movement_ids[movement] = cursor.lastrowid\n",
    "    else:\n",
    "        movement_ids[movement] = result[0]\n",
    "\n",
    "# Process placeholder rows for Results table\n",
    "for movement in ['cmj', 'dj', 'ppu']:\n",
    "    file_path = os.path.join(selected_folder, f\"{movement}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the file and calculate the number of trials dynamically\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            num_trials = len(lines[5:])  # Count rows starting from line 6\n",
    "            for trial_num in range(1, num_trials + 1):\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO Results (Trial_Num, pre_post, name, movement) \n",
    "                    VALUES (?, ?, ?, ?);\n",
    "                \"\"\", (trial_num, pre_post, name, movement))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# At the end of the first code block\n",
    "global_pre_post = pre_post  # Set this as a global variable\n",
    "print(f\"Global pre_post set to: {global_pre_post}\")\n",
    "\n",
    "print(f\"Data for participant '{name}' with test date '{test_date}' has been inserted.\")\n"
   ],
   "id": "c8d95f6af77f7844",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 'Chandler Seagel' assigned to group 'exp' with participant_id: 1\n",
      "Global pre_post set to: post\n",
      "Data for participant 'Chandler Seagel' with test date '2025-01-02' has been inserted.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:16:59.210931Z",
     "start_time": "2025-01-06T23:16:59.160860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def extract_test_date_from_ascii(ascii_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the test date in 'YYYY-MM-DD' format from the first file path in the ASCII file.\n",
    "    \"\"\"\n",
    "    with open(ascii_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Extract the first file path from the first line\n",
    "        first_file_path = lines[0].strip().split('\\t')[0]\n",
    "        parts = first_file_path.split('\\\\')\n",
    "        if len(parts) > 4:\n",
    "            date_folder = parts[4]  # e.g. \"2025-01-02__2\"\n",
    "            match = re.match(r'^\\d{4}-\\d{2}-\\d{2}', date_folder)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unable to extract test date from folder: {date_folder}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected file path structure: Unable to extract test date.\")\n",
    "\n",
    "\n",
    "# Make sure the global_pre_post is available (from code cell 1)\n",
    "if 'global_pre_post' in globals():\n",
    "    pre_post = global_pre_post\n",
    "    print(f\"Using global_pre_post: {pre_post}\")\n",
    "else:\n",
    "    raise ValueError(\"global_pre_post not found. Ensure the first code block was executed.\")\n",
    "\n",
    "\n",
    "db_path = 'D:/Tramp Test/Tramp_Test.sqlite'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Just in case the column doesn't exist in Results\n",
    "try:\n",
    "    cursor.execute(\"\"\"ALTER TABLE Results ADD COLUMN Trial_Num INTEGER;\"\"\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass\n",
    "\n",
    "movements = ['cmj', 'dj', 'ppu']\n",
    "ascii_dir = 'D:/Tramp Test/Output Files/'\n",
    "\n",
    "for movement in movements:\n",
    "    file_path = os.path.join(ascii_dir, f\"{movement}.txt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found for {movement} at: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    test_date = extract_test_date_from_ascii(file_path)\n",
    "    print(f\"\\n>>> Processing {movement.upper()} | Test date: {test_date}\")\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Debug: how many lines\n",
    "    print(f\"Total lines read for {movement}: {len(lines)}\")\n",
    "\n",
    "    # Typically, line[1] has the column names (minus an optional first col).\n",
    "    # But with your example, lines[1] has the 12 metric headers:\n",
    "    #    JH_IN, LEWIS_PEAK_POWER, NORM..., Max_Force, etc... repeated for each trial.\n",
    "    # lines[5] or lines[6] might contain the actual data row(s).\n",
    "\n",
    "    # Let's define the start line for data\n",
    "    data_start_index = 5  # According to your example, real data is at line 6 (index=5).\n",
    "\n",
    "    # We'll read everything from line[1] as the \"header line\"\n",
    "    raw_header = lines[1].strip().split('\\t')\n",
    "    # If there's an initial blank or \"Trial_ID\" column, remove it\n",
    "    # But in your example, it might not exist. Let's check length:\n",
    "    if len(raw_header) > 0 and raw_header[0].strip() == '':\n",
    "        raw_header = raw_header[1:]  # remove the first empty column\n",
    "\n",
    "    print(f\"Header (raw): {raw_header}\")\n",
    "\n",
    "    # We'll skip lines[2], lines[3], lines[4] (METRIC, PROCESSED, ITEM, etc.).\n",
    "    # Then line[5] (index=4) might be \"ITEM X X X ...\", so let's jump to data_start_index=5.\n",
    "\n",
    "    all_data_rows = [line.strip().split('\\t') for line in lines[data_start_index:]]\n",
    "\n",
    "    # Now we expect each row to have 1 + 4*N columns (where N is the number of trials in that row).\n",
    "    # In your example, we have:\n",
    "    #   13 columns total => 1 is the \"Item\" ID + 12 columns for 3 trials of 4 metrics each.\n",
    "\n",
    "    for row_idx, row_data in enumerate(all_data_rows, start=data_start_index):\n",
    "        if not row_data or len(row_data) < 5:\n",
    "            # Possibly an empty line\n",
    "            continue\n",
    "\n",
    "        # For debugging:\n",
    "        print(f\"Row idx={row_idx} => {row_data}\")\n",
    "\n",
    "        # The first column in the row might be the \"Item\" number, e.g. \"1\"\n",
    "        # The rest are sets of 4 columns per trial: [JH_IN, LEWIS_PEAK, NORM_LEWIS, Max_Force]\n",
    "        item_number = row_data[0]  # Often \"1\"\n",
    "\n",
    "        # Let's define the total columns for metric data\n",
    "        metric_cols = row_data[1:]  # skip the item_number\n",
    "        num_metrics = 4  # JH_IN, LEWIS, NORM_LEWIS, Max_Force\n",
    "\n",
    "        # Figure out how many trials are in this row\n",
    "        if len(metric_cols) % num_metrics != 0:\n",
    "            print(f\"Warning: row has {len(metric_cols)} metric cols which is not a multiple of 4.\")\n",
    "        num_trials = len(metric_cols) // num_metrics\n",
    "\n",
    "        # We loop over each chunk of 4 columns as a separate trial\n",
    "        for trial_i in range(num_trials):\n",
    "            start_index = trial_i * num_metrics\n",
    "            end_index   = start_index + num_metrics\n",
    "\n",
    "            # Extract the 4 metrics\n",
    "            jh_in                 = metric_cols[start_index]   if start_index+0 < len(metric_cols) else None\n",
    "            lewis_peak_power      = metric_cols[start_index+1] if start_index+1 < len(metric_cols) else None\n",
    "            norm_lewis_peak_power = metric_cols[start_index+2] if start_index+2 < len(metric_cols) else None\n",
    "            max_force             = metric_cols[start_index+3] if start_index+3 < len(metric_cols) else None\n",
    "\n",
    "            trial_num = trial_i + 1  # numbering trials starting at 1\n",
    "\n",
    "            # Debug printing\n",
    "            print(f\"  Trial #{trial_num} => JH={jh_in}, LEWIS={lewis_peak_power}, \"\n",
    "                  f\"NORM={norm_lewis_peak_power}, F={max_force}\")\n",
    "\n",
    "            # Check if there's already a placeholder row in Results\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT result_id \n",
    "                FROM Results\n",
    "                WHERE name = ? AND movement = ? AND pre_post = ? AND Trial_Num = ?;\n",
    "            \"\"\", (name, movement, pre_post, trial_num))\n",
    "            existing_row = cursor.fetchone()\n",
    "\n",
    "            if existing_row:\n",
    "                # UPDATE\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE Results\n",
    "                    SET JH_IN = ?, \n",
    "                        LEWIS_PEAK_POWER = ?, \n",
    "                        NORM_LEWIS_PEAK_POWER_KG = ?, \n",
    "                        Max_Force = ?\n",
    "                    WHERE result_id = ?;\n",
    "                \"\"\", (\n",
    "                    jh_in,\n",
    "                    lewis_peak_power,\n",
    "                    norm_lewis_peak_power,\n",
    "                    max_force,\n",
    "                    existing_row[0]\n",
    "                ))\n",
    "            else:\n",
    "                # INSERT new row\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO Results (\n",
    "                        Trial_Num, pre_post, name, movement, \n",
    "                        JH_IN, LEWIS_PEAK_POWER, NORM_LEWIS_PEAK_POWER_KG, Max_Force\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\n",
    "                \"\"\", (\n",
    "                    trial_num,\n",
    "                    pre_post,\n",
    "                    name,\n",
    "                    movement,\n",
    "                    jh_in,\n",
    "                    lewis_peak_power,\n",
    "                    norm_lewis_peak_power,\n",
    "                    max_force\n",
    "                ))\n",
    "\n",
    "# Commit once after processing all movements\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nAll trials processed and inserted/updated in the database.\")\n"
   ],
   "id": "63ee9e0450dc78a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global_pre_post: post\n",
      "\n",
      ">>> Processing CMJ | Test date: 2025-01-02\n",
      "Total lines read for cmj: 6\n",
      "Header (raw): ['JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force', 'JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force']\n",
      "Row idx=5 => ['1', '20.61', '8262.50', '92.94', '5588.65', '20.82', '8295.65', '93.31', '5776.83']\n",
      "  Trial #1 => JH=20.61, LEWIS=8262.50, NORM=92.94, F=5588.65\n",
      "  Trial #2 => JH=20.82, LEWIS=8295.65, NORM=93.31, F=5776.83\n",
      "\n",
      ">>> Processing DJ | Test date: 2025-01-02\n",
      "Total lines read for dj: 6\n",
      "Header (raw): ['JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force', 'JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force']\n",
      "Row idx=5 => ['1', '21.89', '8463.91', '95.20', '4149.93', '23.21', '8671.38', '97.54', '5079.83']\n",
      "  Trial #1 => JH=21.89, LEWIS=8463.91, NORM=95.20, F=4149.93\n",
      "  Trial #2 => JH=23.21, LEWIS=8671.38, NORM=97.54, F=5079.83\n",
      "\n",
      ">>> Processing PPU | Test date: 2025-01-02\n",
      "Total lines read for ppu: 6\n",
      "Header (raw): ['JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force', 'JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force', 'JH_IN', 'LEWIS_PEAK_POWER', 'NORM_LEWIS_PEAK_POWER_KG', 'Max_Force']\n",
      "Row idx=5 => ['1', '7.34', '6177.06', '69.48', '4259.86', '6.97', '6118.61', '68.82', '2868.15', '8.93', '6426.03', '72.28', '2216.13']\n",
      "  Trial #1 => JH=7.34, LEWIS=6177.06, NORM=69.48, F=4259.86\n",
      "  Trial #2 => JH=6.97, LEWIS=6118.61, NORM=68.82, F=2868.15\n",
      "  Trial #3 => JH=8.93, LEWIS=6426.03, NORM=72.28, F=2216.13\n",
      "\n",
      "All trials processed and inserted/updated in the database.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T23:54:48.021569Z",
     "start_time": "2025-01-06T23:54:47.698638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Data Loading\n",
    "# ---------------------------\n",
    "db_path = r'D:/Tramp Test/Tramp_Test.sqlite'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Example query that JOINs with Sessions to get test_date. Adjust as needed:\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    R.name,\n",
    "    R.movement,\n",
    "    R.pre_post,\n",
    "    R.JH_IN AS jump_height,\n",
    "    R.LEWIS_PEAK_POWER AS peak_power,\n",
    "    S.test_date\n",
    "FROM Results AS R\n",
    "JOIN Participants AS P\n",
    "    ON R.name = P.name\n",
    "JOIN Sessions AS S\n",
    "    ON P.participant_id = S.participant_id\n",
    "    AND R.pre_post = S.pre_post\n",
    "WHERE R.movement IN ('cmj', 'dj', 'ppu')\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Convert test_date to datetime, if it's not already\n",
    "df['test_date'] = pd.to_datetime(df['test_date'], errors='coerce')\n",
    "\n",
    "# Drop rows without participant name or pre_post\n",
    "df.dropna(subset=['name','pre_post'], inplace=True)\n",
    "\n",
    "participants = df['name'].dropna().unique()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Initialize Dash\n",
    "# ---------------------------\n",
    "app = Dash(__name__)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Layout\n",
    "# ---------------------------\n",
    "# We create 3 rows (CMJ, DJ, PPU), each with 2 columns (Jump Height on the left, Power on the right).\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Pre vs. Post with All Trials + Average Lines\"),\n",
    "\n",
    "    html.Div([\n",
    "        html.Label(\"Select a Participant:\"),\n",
    "        dcc.Dropdown(\n",
    "            id='participant-dropdown',\n",
    "            options=[{'label': p, 'value': p} for p in participants],\n",
    "            value=participants[0] if len(participants) else None,\n",
    "            clearable=False\n",
    "        )\n",
    "    ], style={'width': '30%', 'marginBottom': '20px'}),\n",
    "\n",
    "    # ---------------------------\n",
    "    # CMJ\n",
    "    # ---------------------------\n",
    "    html.H2(\"CMJ\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='cmj-jh-graph', style={'width': '49%', 'display': 'inline-block'}),\n",
    "        dcc.Graph(id='cmj-power-graph', style={'width': '49%', 'display': 'inline-block'})\n",
    "    ], style={'display': 'flex'}),\n",
    "\n",
    "    # ---------------------------\n",
    "    # DJ\n",
    "    # ---------------------------\n",
    "    html.H2(\"DJ\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='dj-jh-graph', style={'width': '49%', 'display': 'inline-block'}),\n",
    "        dcc.Graph(id='dj-power-graph', style={'width': '49%', 'display': 'inline-block'})\n",
    "    ], style={'display': 'flex'}),\n",
    "\n",
    "    # ---------------------------\n",
    "    # PPU\n",
    "    # ---------------------------\n",
    "    html.H2(\"PPU\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='ppu-jh-graph', style={'width': '49%', 'display': 'inline-block'}),\n",
    "        dcc.Graph(id='ppu-power-graph', style={'width': '49%', 'display': 'inline-block'})\n",
    "    ], style={'display': 'flex'}),\n",
    "\n",
    "], style={'margin': '20px'})\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Callback\n",
    "# ---------------------------\n",
    "@app.callback(\n",
    "    [\n",
    "        Output('cmj-jh-graph','figure'),\n",
    "        Output('cmj-power-graph','figure'),\n",
    "        Output('dj-jh-graph','figure'),\n",
    "        Output('dj-power-graph','figure'),\n",
    "        Output('ppu-jh-graph','figure'),\n",
    "        Output('ppu-power-graph','figure'),\n",
    "    ],\n",
    "    [Input('participant-dropdown','value')]\n",
    ")\n",
    "def update_plots(selected_participant):\n",
    "    \"\"\"For each movement (CMJ, DJ, PPU), create two figures:\n",
    "       1) Jump Height\n",
    "       2) Peak Power\n",
    "\n",
    "       Each figure:\n",
    "         - Plots raw points for each session's pre/post\n",
    "         - Plots a dashed line from mean(pre) to mean(post)\n",
    "         - Each session (test_date) gets a different color\n",
    "         - 90% opacity\n",
    "         - Black background, gray plot area, white text\n",
    "    \"\"\"\n",
    "    dff = df[df['name'] == selected_participant].copy()\n",
    "    \n",
    "    # For convenience, define a function that builds a single figure for a movement + column.\n",
    "    def build_pre_post_figure(movement_label, y_col):\n",
    "        \"\"\"\n",
    "        movement_label: 'cmj', 'dj', 'ppu'\n",
    "        y_col: 'jump_height' or 'peak_power'\n",
    "\n",
    "        Returns a Figure with:\n",
    "          - All raw points for pre/post\n",
    "          - A dashed line connecting average(pre) -> average(post)\n",
    "          - One color per test_date\n",
    "        \"\"\"\n",
    "        sub_df = dff[dff['movement'] == movement_label].copy()\n",
    "        # Sort by test_date so each test_date has a consistent color order\n",
    "        sub_df.sort_values('test_date', inplace=True)\n",
    "\n",
    "        # Unique test sessions (dates). We rely on Plotly's default color cycle\n",
    "        # so each date gets a different color automatically.\n",
    "        unique_dates = sub_df['test_date'].dropna().unique()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for session_date in unique_dates:\n",
    "            # Filter the data for that session_date\n",
    "            sess_mask = (sub_df['test_date'] == session_date)\n",
    "            sess_data = sub_df[sess_mask]\n",
    "            if sess_data.empty:\n",
    "                continue\n",
    "\n",
    "            # Split into pre and post\n",
    "            pre_data = sess_data[sess_data['pre_post'].str.lower()=='pre']\n",
    "            post_data= sess_data[sess_data['pre_post'].str.lower()=='post']\n",
    "\n",
    "            # 1) Raw points for PRE\n",
    "            if not pre_data.empty:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=['pre']*len(pre_data),\n",
    "                    y=pre_data[y_col],\n",
    "                    mode='markers',\n",
    "                    name=f\"{session_date.date()} (pre) pts\",\n",
    "                    opacity=0.9,\n",
    "                    # We'll let Plotly handle the color,\n",
    "                    # but you could specify one if you want:\n",
    "                    # marker=dict(color='rgba(255,0,0,0.9)'),\n",
    "                ))\n",
    "\n",
    "            # 2) Raw points for POST\n",
    "            if not post_data.empty:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=['post']*len(post_data),\n",
    "                    y=post_data[y_col],\n",
    "                    mode='markers',\n",
    "                    name=f\"{session_date.date()} (post) pts\",\n",
    "                    opacity=0.9\n",
    "                ))\n",
    "\n",
    "            # 3) Dashed line from mean(pre) to mean(post), no markers\n",
    "            if (not pre_data.empty) and (not post_data.empty):\n",
    "                avg_pre  = pre_data[y_col].mean()\n",
    "                avg_post = post_data[y_col].mean()\n",
    "                if not np.isnan(avg_pre) and not np.isnan(avg_post):\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=['pre','post'],\n",
    "                        y=[avg_pre, avg_post],\n",
    "                        mode='lines',\n",
    "                        line=dict(dash='dash', width=2),  # dashed line\n",
    "                        opacity=0.9,\n",
    "                        showlegend=False  # We don't want a separate legend entry for the line\n",
    "                    ))\n",
    "\n",
    "        # Update layout for dark background, etc.\n",
    "        fig.update_layout(\n",
    "            paper_bgcolor='black',   # outside the plot\n",
    "            plot_bgcolor='grey',     # behind the data\n",
    "            font=dict(color='white'),\n",
    "            xaxis=dict(title=\"Pre vs Post\", type='category'),\n",
    "            yaxis=dict(title=y_col.replace('_',' ').title()),\n",
    "            hovermode='closest'  # or 'x unified'\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    # Build 6 figures\n",
    "    cmj_jh_fig    = build_pre_post_figure('cmj', 'jump_height')\n",
    "    cmj_power_fig = build_pre_post_figure('cmj', 'peak_power')\n",
    "    dj_jh_fig     = build_pre_post_figure('dj',  'jump_height')\n",
    "    dj_power_fig  = build_pre_post_figure('dj',  'peak_power')\n",
    "    ppu_jh_fig    = build_pre_post_figure('ppu', 'jump_height')\n",
    "    ppu_power_fig = build_pre_post_figure('ppu', 'peak_power')\n",
    "\n",
    "    return (\n",
    "        cmj_jh_fig,\n",
    "        cmj_power_fig,\n",
    "        dj_jh_fig,\n",
    "        dj_power_fig,\n",
    "        ppu_jh_fig,\n",
    "        ppu_power_fig\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Run the App\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ],
   "id": "75c0647a5409497d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x12d5c188d70>"
      ],
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T21:53:42.302767Z",
     "start_time": "2025-01-06T21:53:42.184245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reorders the database to be in alphabetical order\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "db_path = \"D:/Tramp Test/Tramp_Test.sqlite\" \n",
    "sort_column = \"name\"     \n",
    "\n",
    "def reorder_all_tables(db_path, sort_column):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Fetch all table names in the database\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "\n",
    "            # Skip system tables like sqlite_sequence\n",
    "            if table_name.startswith(\"sqlite_\"):\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing table: {table_name}\")\n",
    "\n",
    "            # Check if the column exists in the current table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = [info[1] for info in cursor.fetchall()]\n",
    "            if sort_column not in columns:\n",
    "                print(f\"Skipping table '{table_name}' - Column '{sort_column}' not found.\")\n",
    "                continue\n",
    "\n",
    "            # Create a new sorted table\n",
    "            temp_table = f\"{table_name}_sorted\"\n",
    "            cursor.execute(f\"CREATE TABLE {temp_table} AS SELECT * FROM {table_name} ORDER BY {sort_column} ASC;\")\n",
    "            \n",
    "            # Drop the old table\n",
    "            cursor.execute(f\"DROP TABLE {table_name};\")\n",
    "            \n",
    "            # Rename the new table to the original name\n",
    "            cursor.execute(f\"ALTER TABLE {temp_table} RENAME TO {table_name};\")\n",
    "            print(f\"Table '{table_name}' reordered successfully.\")\n",
    "\n",
    "        # Commit changes\n",
    "        conn.commit()\n",
    "        print(\"All tables processed.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "reorder_all_tables(db_path, sort_column)\n"
   ],
   "id": "e37ec20f869930f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: Participants\n",
      "Table 'Participants' reordered successfully.\n",
      "Processing table: Sessions\n",
      "Skipping table 'Sessions' - Column 'name' not found.\n",
      "Processing table: Movements\n",
      "Skipping table 'Movements' - Column 'name' not found.\n",
      "Processing table: Results\n",
      "Table 'Results' reordered successfully.\n",
      "All tables processed.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
